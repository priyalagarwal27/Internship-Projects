{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEB SCRAPING ASSIGNMENT - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import urllib\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import time,json\n",
    "from PIL import Image\n",
    "import io,os\n",
    "import re\n",
    "import hashlib\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "from selenium.common.exceptions import NoSuchElementException,ElementNotVisibleException,StaleElementReferenceException,ElementClickInterceptedException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/ \n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Name Rank  \\\n",
      "0                            \"Baby Shark Dance\" 28     1   \n",
      "1                                   \"Despacito\" 30     2   \n",
      "2                                \"Shape of You\" 31     3   \n",
      "3                               \"See You Again\" 32     4   \n",
      "4                        \"Johny Johny Yes Papa\" 35     5   \n",
      "5    \"Masha and the Bear – Recipe for Disaster\" 36     6   \n",
      "6                                 \"Uptown Funk\" 37     7   \n",
      "7                               \"Gangnam Style\" 38     8   \n",
      "8   \"Learning Colors – Colorful Eggs on a Farm\" 40     9   \n",
      "9                                   \"Bath Song\" 41    10   \n",
      "10                \"Phonics Song with Two Words\" 42    11   \n",
      "11                                      \"Sorry\" 43    12   \n",
      "12                                      \"Sugar\" 44    13   \n",
      "13                                       \"Roar\" 45    14   \n",
      "14                             \"Counting Stars\" 46    15   \n",
      "15                          \"Thinking Out Loud\" 47    16   \n",
      "16                             \"Dame Tu Cosita\" 48    17   \n",
      "17                               \"Shake It Off\" 49    18   \n",
      "18                                      \"Faded\" 50    19   \n",
      "19                                    \"Lean On\" 51    20   \n",
      "20                                   \"Bailando\" 52    21   \n",
      "21                                 \"Dark Horse\" 53    22   \n",
      "22                             \"Girls Like You\" 54    23   \n",
      "23                                 \"Let Her Go\" 55    24   \n",
      "24                                   \"Mi Gente\" 56    25   \n",
      "25                                      \"Hello\" 57    26   \n",
      "26                                    \"Perfect\" 58    27   \n",
      "27           \"Waka Waka (This Time for Africa)\" 59    28   \n",
      "28                                \"Blank Space\" 60    29   \n",
      "29                                   \"Chantaje\" 61    30   \n",
      "\n",
      "                                               Artist               Date Views  \n",
      "0                      Pinkfong Kids' Songs & Stories      June 17, 2016  7.91  \n",
      "1                   Luis Fonsi featuring Daddy Yankee   January 12, 2017  7.20  \n",
      "2                                          Ed Sheeran   January 30, 2017  5.18  \n",
      "3                  Wiz Khalifa featuring Charlie Puth      April 6, 2015  4.96  \n",
      "4                                         LooLoo Kids    October 8, 2016  4.77  \n",
      "5                                          Get Movies   January 31, 2012  4.41  \n",
      "6                    Mark Ronson featuring Bruno Mars  November 19, 2014  4.09  \n",
      "7                                                 Psy      July 15, 2012  3.97  \n",
      "8                                         Miroshka TV  February 27, 2018  3.69  \n",
      "9                          Cocomelon – Nursery Rhymes        May 2, 2018  3.64  \n",
      "10                                          ChuChu TV      March 6, 2014  3.53  \n",
      "11                                      Justin Bieber   October 22, 2015  3.40  \n",
      "12                                           Maroon 5   January 14, 2015  3.39  \n",
      "13                                         Katy Perry  September 5, 2013  3.27  \n",
      "14                                        OneRepublic       May 31, 2013  3.19  \n",
      "15                                         Ed Sheeran    October 7, 2014  3.18  \n",
      "16                    El Chombo featuring Cutty Ranks      April 5, 2018  3.11  \n",
      "17                                       Taylor Swift    August 18, 2014  3.02  \n",
      "18                                        Alan Walker   December 3, 2015  2.98  \n",
      "19              Major Lazer and DJ Snake featuring MØ     March 22, 2015  2.97  \n",
      "20  Enrique Iglesias featuring Descemer Bueno and ...     April 11, 2014  2.97  \n",
      "21                       Katy Perry featuring Juicy J  February 20, 2014  2.97  \n",
      "22                         Maroon 5 featuring Cardi B       May 31, 2018  2.96  \n",
      "23                                          Passenger      July 25, 2012  2.91  \n",
      "24                         J Balvin and Willy William      June 29, 2017  2.86  \n",
      "25                                              Adele   October 22, 2015  2.79  \n",
      "26                                         Ed Sheeran   November 9, 2017  2.74  \n",
      "27                    Shakira featuring Freshlyground       June 4, 2010  2.74  \n",
      "28                                       Taylor Swift  November 10, 2014  2.72  \n",
      "29                           Shakira featuring Maluma  November 18, 2016  2.62  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "driver.get('https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos')\n",
    "time.sleep(2)\n",
    "\n",
    "#Scroll upto target location\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "target = driver.find_element_by_xpath('/html/body/div[3]/div[3]/div[5]/div[1]/h2[2]/span')\n",
    "target.location_once_scrolled_into_view \n",
    "time.sleep(3)\n",
    "\n",
    "#empty list\n",
    "rank,name,artist,date,views=[],[],[],[],[]\n",
    "\n",
    "#Scrape required details \n",
    "Name = driver.find_elements_by_xpath(\"/html/body/div[3]/div[3]/div[5]/div[1]/table[1]/tbody/tr/td[2]\")\n",
    "for i in Name:\n",
    "    name.append(i.text.replace('[',\" \").replace(']',' ').rstrip(''))\n",
    "                \n",
    "Rank=driver.find_elements_by_xpath(\"/html/body/div[3]/div[3]/div[5]/div[1]/table[1]/tbody/tr/td[1]\")\n",
    "for i in Rank:\n",
    "    rank.append(i.text.rstrip(\".\"))\n",
    "    \n",
    "Artist = driver.find_elements_by_xpath(\"/html/body/div[3]/div[3]/div[5]/div[1]/table[1]/tbody/tr/td[3]\")\n",
    "for i in Artist:\n",
    "    artist.append(i.text)\n",
    "        \n",
    "Date = driver.find_elements_by_xpath(\"/html/body/div[3]/div[3]/div[5]/div[1]/table[1]/tbody/tr/td[5]\")\n",
    "for i in Date:\n",
    "    date.append(i.text)\n",
    "    \n",
    "Views = driver.find_elements_by_xpath(\"/html/body/div[3]/div[3]/div[5]/div[1]/table[1]/tbody/tr/td[4]\")\n",
    "for i in Views:\n",
    "    views.append(i.text)\n",
    "    \n",
    "driver.quit()   \n",
    "#print(len(Name),len(Year),len(Genre),len(Run_time),len(Ratings),len(Votes))\n",
    "\n",
    "#Saving in dataframe            \n",
    "df=pd.DataFrame({'Name': name,\n",
    "                 'Rank':rank,\n",
    "                 'Artist':artist,\n",
    "                 'Date' : date,\n",
    "                 'Views':views})\n",
    "print(df)\n",
    "df.to_csv('youtube.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2.Scrape the details team India’s international fixtures from bcci.tv. Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Title               Date       Time  \\\n",
      "0  1st ODI  Tuesday,23,MARCH,  13:30 IST   \n",
      "1  2nd ODI   Friday,26,MARCH,  13:30 IST   \n",
      "2  3rd ODI   Sunday,28,MARCH,  13:30 IST   \n",
      "\n",
      "                                           Place                Series  \n",
      "0  Maharashtra Cricket Association Stadium, Pune  INDIA V ENGLAND 2021  \n",
      "1  Maharashtra Cricket Association Stadium, Pune  INDIA V ENGLAND 2021  \n",
      "2  Maharashtra Cricket Association Stadium, Pune  INDIA V ENGLAND 2021  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "driver.get('https://www.bcci.tv/')\n",
    "time.sleep(3)\n",
    "\n",
    "#Click on International\n",
    "driver.find_element_by_xpath('/html/body/div[3]/div/div[2]/div[2]/nav/ul/li[1]/div[2]').click()\n",
    "time.sleep(3)\n",
    "#Click on Fixtures\n",
    "driver.find_element_by_xpath('/html/body/div[3]/div/div[2]/div[2]/nav/ul/li[1]/div[2]/div/ul/li[1]/a').click()\n",
    "time.sleep(2)\n",
    "#Click on 'matches in' arrow button\n",
    "driver.find_element_by_xpath('//*[@id=\"TestAlexFilter\"]').click()\n",
    "time.sleep(2)\n",
    "#Click on ODI\n",
    "driver.find_element_by_xpath('/html/body/div[4]/div/div/div[2]/div/div/div[3]/div/div[2]/div/ul/li[4]/span').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Creating empty list\n",
    "title,series,place,date,time = [],[],[],[],[]\n",
    "\n",
    "#Scrape required details\n",
    "Place=driver.find_elements_by_xpath(\"/html/body/div[4]/div/div/div[2]/section/div/div/a/div[2]/div[2]/p/span\")\n",
    "for i in Place:\n",
    "    place.append(i.text)\n",
    "    \n",
    "Time=driver.find_elements_by_xpath(\"/html/body/div[4]/div/div/div[2]/section/div/div/a/div[1]/div/div/span[2]\")\n",
    "for i in Time:\n",
    "    time.append(i.text)\n",
    "\n",
    "Title=driver.find_elements_by_xpath(\"/html/body/div[4]/div/div/div[2]/section/div/div/a/div[2]/div[2]/p/strong\")\n",
    "for i in Title:\n",
    "    title.append(i.text)\n",
    "\n",
    "Series=driver.find_elements_by_xpath('/html/body/div[4]/div/div/div[2]/section/div/div/a/div[2]/div[1]/span[2]')\n",
    "for i in Series:\n",
    "    series.append(i.text)\n",
    "    \n",
    "Date=driver.find_elements_by_xpath(\"//div[@class='fixture__datetime desktop-only']\") \n",
    "for i in Date:\n",
    "    date.append(i.text.replace('\\n',',').rstrip(\"13:30 IST\"))\n",
    "    \n",
    "driver.quit()   \n",
    "#print(len(title),len(place),len(date),len(time),len(series))\n",
    "#Saving in dataframe            \n",
    "df=pd.DataFrame({'Title': title,\n",
    "                 'Date':date,\n",
    "                 'Time':time,\n",
    "                 'Place' : place,\n",
    "                 'Series':series})\n",
    "print(df)\n",
    "df.to_csv('bcci.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.Scrape the details of selenium exception from guru99.com. Url = https://www.guru99.com/\n",
    "You need to find following details:\n",
    "A) Name\n",
    "B) Description\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Exception Name  \\\n",
      "0            ElementNotVisibleException   \n",
      "1         ElementNotSelectableException   \n",
      "2                NoSuchElementException   \n",
      "3                  NoSuchFrameException   \n",
      "4               NoAlertPresentException   \n",
      "5                 NoSuchWindowException   \n",
      "6        StaleElementReferenceException   \n",
      "7              SessionNotFoundException   \n",
      "8                      TimeoutException   \n",
      "9                    WebDriverException   \n",
      "10            ConnectionClosedException   \n",
      "11     ElementClickInterceptedException   \n",
      "12      ElementNotInteractableException   \n",
      "13             ErrorInResponseException   \n",
      "14  ErrorHandler.UnknownServerException   \n",
      "15         ImeActivationFailedException   \n",
      "16             ImeNotAvailableException   \n",
      "17         InsecureCertificateException   \n",
      "18             InvalidArgumentException   \n",
      "19         InvalidCookieDomainException   \n",
      "20          InvalidCoordinatesException   \n",
      "21          InvalidElementStateExceptio   \n",
      "22            InvalidSessionIdException   \n",
      "23       InvalidSwitchToTargetException   \n",
      "24                  JavascriptException   \n",
      "25                        JsonException   \n",
      "26             NoSuchAttributeException   \n",
      "27       MoveTargetOutOfBoundsException   \n",
      "28               NoSuchContextException   \n",
      "29                NoSuchCookieException   \n",
      "30                    NotFoundException   \n",
      "31          RemoteDriverServerException   \n",
      "32                  ScreenshotException   \n",
      "33           SessionNotCreatedException   \n",
      "34           UnableToSetCookieException   \n",
      "35           UnexpectedTagNameException   \n",
      "36              UnhandledAlertException   \n",
      "37      UnexpectedAlertPresentException   \n",
      "38               UnknownMethodException   \n",
      "39          UnreachableBrowserException   \n",
      "40          UnsupportedCommandException   \n",
      "\n",
      "                                          Description  \n",
      "0   This type of Selenium exception occurs when an...  \n",
      "1   This Selenium exception occurs when an element...  \n",
      "2   This Exception occurs if an element could not ...  \n",
      "3   This Exception occurs if the frame target to b...  \n",
      "4   This Exception occurs when you switch to no pr...  \n",
      "5   This Exception occurs if the window target to ...  \n",
      "6   This Selenium exception occurs happens when th...  \n",
      "7   The WebDriver is acting after you quit the bro...  \n",
      "8   Thrown when there is not enough time for a com...  \n",
      "9   This Exception takes place when the WebDriver ...  \n",
      "10  This type of Exception takes place when there ...  \n",
      "11  The command may not be completed as the elemen...  \n",
      "12  This Selenium exception is thrown when any ele...  \n",
      "13  This happens while interacting with the Firefo...  \n",
      "14  Exception is used as a placeholder in case if ...  \n",
      "15  This expectation will occur when IME engine ac...  \n",
      "16    It takes place when IME support is unavailable.  \n",
      "17  Navigation made the user agent to hit a certif...  \n",
      "18  It occurs when an argument does not belong to ...  \n",
      "19  This happens when you try to add a cookie unde...  \n",
      "20  This type of Exception matches an interacting ...  \n",
      "21  It occurs when command can't be finished when ...  \n",
      "22  This Exception took place when the given sessi...  \n",
      "23  This occurs when the frame or window target to...  \n",
      "24  This issue occurs while executing JavaScript g...  \n",
      "25  It occurs when you afford to get the session w...  \n",
      "26  This kind of Exception occurs when the attribu...  \n",
      "27  It takes place if the target provided to the A...  \n",
      "28           ContextAware does mobile device testing.  \n",
      "29  This Exception occurs when no cookie matching ...  \n",
      "30  This Exception is a subclass of WebDriverExcep...  \n",
      "31  This Selenium exception is thrown when the ser...  \n",
      "32            It is not possible to capture a screen.  \n",
      "33  It happens when a new session could not be suc...  \n",
      "34  This occurs if a driver is unable to set a coo...  \n",
      "35  Happens if a support class did not get a web e...  \n",
      "36  This expectation occurs when there is an alert...  \n",
      "37  It occurs when there is the appearance of an u...  \n",
      "38  This Exception happens when the requested comm...  \n",
      "39  This Exception occurs only when the browser is...  \n",
      "40  This occurs when remote WebDriver does n't sen...  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "driver.get('https://www.guru99.com/')\n",
    "time.sleep(3)\n",
    "\n",
    "#Click on selenium\n",
    "driver.find_element_by_xpath('/html/body/div[2]/section[4]/div/div/div/div/div/div/div/div[1]/div/ul[1]/li[3]/a').click()\n",
    "\n",
    "#Scroll to target location\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "target = driver.find_element_by_xpath('/html/body/div[2]/section[3]/div/div[1]/main/div/div/div/div/div/div/div[2]/table[5]/tbody/tr[34]/td[2]')\n",
    "target.location_once_scrolled_into_view \n",
    "time.sleep(3)\n",
    "\n",
    "#Click on Selenium Exception  \n",
    "driver.find_element_by_xpath('/html/body/div[2]/section[3]/div/div[1]/main/div/div/div/div/div/div/div[2]/table[5]/tbody/tr[34]/td[1]/a/strong').click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Scroll to target location \n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "target = driver.find_element_by_xpath('/html/body/div[2]/section[3]/div/div[1]/main/div[1]/div/div/div/div/div/div[2]/h2[1]')\n",
    "target.location_once_scrolled_into_view \n",
    "time.sleep(3)\n",
    "\n",
    "#empty list\n",
    "name,desc=[],[]\n",
    "\n",
    "#loop to scrape details \n",
    "for i in driver.find_elements_by_xpath(\"/html/body/div[2]/section[3]/div/div[1]/main/div[1]/div/div/div/div/div/div[2]/table/tbody/tr/td[1]\"):\n",
    "    if i.text is None :\n",
    "        name.append(\"--\") \n",
    "    else:\n",
    "        name.append(i.text)\n",
    "               \n",
    "for i in driver.find_elements_by_xpath(\"/html/body/div[2]/section[3]/div/div[1]/main/div[1]/div/div/div/div/div/div[2]/table/tbody/tr/td[2]\"):\n",
    "    if i.text is None :\n",
    "        desc.append(\"--\") \n",
    "    else:\n",
    "        desc.append(i.text)\n",
    "\n",
    "driver.quit()\n",
    "#print(len(name),len(desc))\n",
    "#Saving in Dataframe\n",
    "df=pd.DataFrame({'Exception Name':name[1:],\n",
    "                 'Description':desc[1:]})\n",
    "\n",
    "print(df)\n",
    "df.to_csv(\"exception.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.Scrape the details of State-wise GDP of India from statisticstime.com. Url = http://statisticstimes.com/ \n",
    "You have to find following details:\n",
    "A) Rank \n",
    "B) State \n",
    "C) GSDP(18-19) \n",
    "D) GSDP(17-18)\n",
    "E) Share(2017) \n",
    "F) GDP($ billion) \n",
    "Note: - From statisticstimes home page you have to reach to economy page through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>Share In GDP</th>\n",
       "      <th>GDP of State</th>\n",
       "      <th>GSDP_Current</th>\n",
       "      <th>GSDP_Previous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>13.88%</td>\n",
       "      <td>398.145</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>2,039,074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>8.79%</td>\n",
       "      <td>252.278</td>\n",
       "      <td>1,668,229</td>\n",
       "      <td>1,137,469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>8.59%</td>\n",
       "      <td>246.529</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,215,307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>8.14%</td>\n",
       "      <td>233.552</td>\n",
       "      <td>1,544,399</td>\n",
       "      <td>1,124,423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>7.92%</td>\n",
       "      <td>227.276</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>1,186,379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>5.75%</td>\n",
       "      <td>164.820</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>739,525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>4.97%</td>\n",
       "      <td>142.543</td>\n",
       "      <td>942,586</td>\n",
       "      <td>677,428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>4.55%</td>\n",
       "      <td>130.501</td>\n",
       "      <td>862,957</td>\n",
       "      <td>621,301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>4.54%</td>\n",
       "      <td>130.210</td>\n",
       "      <td>861,031</td>\n",
       "      <td>612,828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>4.27%</td>\n",
       "      <td>122.431</td>\n",
       "      <td>809,592</td>\n",
       "      <td>522,009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>4.12%</td>\n",
       "      <td>118.206</td>\n",
       "      <td>781,653</td>\n",
       "      <td>559,412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>4.08%</td>\n",
       "      <td>117.180</td>\n",
       "      <td>774,870</td>\n",
       "      <td>590,569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>3.87%</td>\n",
       "      <td>111.024</td>\n",
       "      <td>734,163</td>\n",
       "      <td>531,085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>2.80%</td>\n",
       "      <td>80.204</td>\n",
       "      <td>530,363</td>\n",
       "      <td>375,651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>2.77%</td>\n",
       "      <td>79.601</td>\n",
       "      <td>526,376</td>\n",
       "      <td>397,669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>2.59%</td>\n",
       "      <td>74.437</td>\n",
       "      <td>492,229</td>\n",
       "      <td>382,218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.769</td>\n",
       "      <td>315,881</td>\n",
       "      <td>234,048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>1.60%</td>\n",
       "      <td>45.982</td>\n",
       "      <td>304,063</td>\n",
       "      <td>231,182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>44.945</td>\n",
       "      <td>297,204</td>\n",
       "      <td>224,986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.186</td>\n",
       "      <td>245,895</td>\n",
       "      <td>193,273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>0.82%</td>\n",
       "      <td>23.584</td>\n",
       "      <td>155,956</td>\n",
       "      <td>112,755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.265</td>\n",
       "      <td>153,845</td>\n",
       "      <td>117,851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.065</td>\n",
       "      <td>73,170</td>\n",
       "      <td>62,539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.538</td>\n",
       "      <td>49,845</td>\n",
       "      <td>36,963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.369</td>\n",
       "      <td>42,114</td>\n",
       "      <td>31,192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>0.19%</td>\n",
       "      <td>5.543</td>\n",
       "      <td>36,656</td>\n",
       "      <td>24,442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.063</td>\n",
       "      <td>33,481</td>\n",
       "      <td>24,682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.344</td>\n",
       "      <td>28,723</td>\n",
       "      <td>18,722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.214</td>\n",
       "      <td>27,869</td>\n",
       "      <td>19,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.126</td>\n",
       "      <td>27,283</td>\n",
       "      <td>17,647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.721</td>\n",
       "      <td>24,603</td>\n",
       "      <td>16,676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>0.10%</td>\n",
       "      <td>2.952</td>\n",
       "      <td>19,520</td>\n",
       "      <td>14,524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State Share In GDP GDP of State GSDP_Current  \\\n",
       "0     1                Maharashtra       13.88%      398.145    2,632,792   \n",
       "1     2              Uttar Pradesh        8.79%      252.278    1,668,229   \n",
       "2     3                 Tamil Nadu        8.59%      246.529    1,630,208   \n",
       "3     4                  Karnataka        8.14%      233.552    1,544,399   \n",
       "4     5                    Gujarat        7.92%      227.276    1,502,899   \n",
       "5     6                West Bengal        5.75%      164.820    1,089,898   \n",
       "6     7                  Rajasthan        4.97%      142.543      942,586   \n",
       "7     8             Andhra Pradesh        4.55%      130.501      862,957   \n",
       "8     9                  Telangana        4.54%      130.210      861,031   \n",
       "9    10             Madhya Pradesh        4.27%      122.431      809,592   \n",
       "10   11                     Kerala        4.12%      118.206      781,653   \n",
       "11   12                      Delhi        4.08%      117.180      774,870   \n",
       "12   13                    Haryana        3.87%      111.024      734,163   \n",
       "13   14                      Bihar        2.80%       80.204      530,363   \n",
       "14   15                     Punjab        2.77%       79.601      526,376   \n",
       "15   16                     Odisha        2.59%       74.437      492,229   \n",
       "16   17                      Assam        1.67%       47.769      315,881   \n",
       "17   18               Chhattisgarh        1.60%       45.982      304,063   \n",
       "18   19                  Jharkhand        1.57%       44.945      297,204   \n",
       "19   20                Uttarakhand        1.30%       37.186      245,895   \n",
       "20   21            Jammu & Kashmir        0.82%       23.584      155,956   \n",
       "21   22           Himachal Pradesh        0.81%       23.265      153,845   \n",
       "22   23                        Goa        0.39%       11.065       73,170   \n",
       "23   24                    Tripura        0.26%        7.538       49,845   \n",
       "24   25                 Chandigarh        0.22%        6.369       42,114   \n",
       "25   26                 Puducherry        0.19%        5.543       36,656   \n",
       "26   27                  Meghalaya        0.18%        5.063       33,481   \n",
       "27   28                     Sikkim        0.15%        4.344       28,723   \n",
       "28   29                    Manipur        0.15%        4.214       27,869   \n",
       "29   30                   Nagaland        0.14%        4.126       27,283   \n",
       "30   31          Arunachal Pradesh        0.13%        3.721       24,603   \n",
       "31   32                    Mizoram        0.10%        2.952       19,520   \n",
       "32   33  Andaman & Nicobar Islands            -            -            -   \n",
       "\n",
       "   GSDP_Previous  \n",
       "0      2,039,074  \n",
       "1      1,137,469  \n",
       "2      1,215,307  \n",
       "3      1,124,423  \n",
       "4      1,186,379  \n",
       "5        739,525  \n",
       "6        677,428  \n",
       "7        621,301  \n",
       "8        612,828  \n",
       "9        522,009  \n",
       "10       559,412  \n",
       "11       590,569  \n",
       "12       531,085  \n",
       "13       375,651  \n",
       "14       397,669  \n",
       "15       382,218  \n",
       "16       234,048  \n",
       "17       231,182  \n",
       "18       224,986  \n",
       "19       193,273  \n",
       "20       112,755  \n",
       "21       117,851  \n",
       "22        62,539  \n",
       "23        36,963  \n",
       "24        31,192  \n",
       "25        24,442  \n",
       "26        24,682  \n",
       "27        18,722  \n",
       "28        19,300  \n",
       "29        17,647  \n",
       "30        16,676  \n",
       "31        14,524  \n",
       "32             -  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "driver.get('http://statisticstimes.com/')\n",
    "time.sleep(3)\n",
    "\n",
    "#Empty list\n",
    "Rank=[]\n",
    "State =[]\n",
    "GDP=[]\n",
    "GSDP_Current=[]\n",
    "GSDP_Previous=[]\n",
    "Share=[]\n",
    "\n",
    "#Click on economy option\n",
    "driver.find_element_by_xpath('//*[@id=\"top\"]/div[2]/div[2]/button').click()\n",
    "driver.find_element_by_xpath('//*[@id=\"top\"]/div[2]/div[2]/div/a[3]').click()\n",
    "#Click on GDP India option\n",
    "driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[2]/ul/li[1]/a').click()\n",
    "\n",
    "#Scrape required details\n",
    "for i in driver.find_elements_by_xpath(\"//td[@class='data1']\"):\n",
    "    if i.text is None :\n",
    "        Rank.append(\"--\") \n",
    "    else:\n",
    "        Rank.append(i.text)\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//td[@class='name']\"):\n",
    "    if i.text is None :\n",
    "        State.append(\"--\") \n",
    "    else:\n",
    "        State.append(i.text)\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr/td[6]\"):\n",
    "    if i.text is None :\n",
    "        GDP.append(\"--\") \n",
    "    else:\n",
    "        GDP.append(i.text)\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr/td[5]\"):\n",
    "    if i.text is None :\n",
    "        Share.append(\"--\") \n",
    "    else:\n",
    "        Share.append(i.text)\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr/td[4]\"):\n",
    "    if i.text is None :\n",
    "        GSDP_Current.append(\"--\") \n",
    "    else:\n",
    "        GSDP_Current.append(i.text)\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"//*[@id='table_id']/tbody/tr/td[8]\"):\n",
    "    if i.text is None :\n",
    "        GSDP_Previous.append(\"--\") \n",
    "    else:\n",
    "        GSDP_Previous.append(i.text)\n",
    "\n",
    "driver.quit()\n",
    "#Saving in Dataframe\n",
    "State_GDP=pd.DataFrame([])\n",
    "State_GDP['Rank']=Rank[:33]\n",
    "State_GDP['State']=State[:33]\n",
    "State_GDP['Share In GDP']=Share[:33]\n",
    "State_GDP['GDP of State']=GDP[:33]\n",
    "State_GDP['GSDP_Current']=GSDP_Current[:33]\n",
    "State_GDP['GSDP_Previous']=GSDP_Previous[:33]\n",
    "State_GDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Scrape the details of trending repositories on Github.com. Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Repository Title                             Repository Description  \\\n",
      "0      GTAmodding /                                 GTA III, Vice City   \n",
      "1         benawad /                  The home for voice conversations.   \n",
      "2       ratfactor /  Learn the Zig programming language by fixing t...   \n",
      "3      bevyengine /  A refreshingly simple data-driven game engine ...   \n",
      "4   adityatelange /               A fast, clean, responsive Hugo theme   \n",
      "5  Melvin-Abraham /  A cross-platform unofficial Google Assistant C...   \n",
      "6          ageron /  A series of Jupyter notebooks that walk you th...   \n",
      "7           vlang /  Simple, fast, safe, compiled language for deve...   \n",
      "8   flameshot-org /  Powerful yet simple to use screenshot software...   \n",
      "9        skerkour /  The simplest way to de-Google your life and bu...   \n",
      "\n",
      "  Contributors Count     Language used  \n",
      "0                171               C++  \n",
      "1                135        TypeScript  \n",
      "2                 22               Zig  \n",
      "3                524              Rust  \n",
      "4                248              HTML  \n",
      "5                 61        JavaScript  \n",
      "6              6,260  Jupyter Notebook  \n",
      "7              1,335                 V  \n",
      "8                603               C++  \n",
      "9                 14              Rust  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "driver.get('https://github.com/')\n",
    "driver.maximize_window()\n",
    "time.sleep(3)\n",
    "\n",
    "#Click for Explore option\n",
    "driver.find_element_by_xpath('/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Click for trending option\n",
    "driver.find_element_by_xpath('/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]').click()\n",
    "time.sleep(3)\n",
    "\n",
    "title,desc,count,lang =[],[],[],[]\n",
    "#loop to scrape details \n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='text-normal']\"):\n",
    "    title.append(i.text.replace('\\n',' '))              \n",
    "                                      \n",
    "for i in driver.find_elements_by_xpath(\"//p[@class='col-9 text-gray my-1 pr-4']\"):\n",
    "    desc.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath(\"//span[@itemprop='programmingLanguage']\"):\n",
    "    if i.text is None:\n",
    "        lang.append(\"--\")\n",
    "    else:\n",
    "        lang.append(i.text)\n",
    "\n",
    "#scraping the Muted_Link And Star \n",
    "Count=driver.find_elements_by_xpath(\"//a[@class='muted-link d-inline-block mr-3']\")\n",
    "for i in Count:\n",
    "    if i.text is None :\n",
    "        count.append(\"--\") \n",
    "    else:\n",
    "        count.append(i.text)\n",
    "Counted=[]\n",
    "for i in range(1,len(count),2):\n",
    "    Counted.append(count[i])\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "#print(len(title),len(desc),len(count),len(lang))\n",
    "#Saving in Dataframe\n",
    "df=pd.DataFrame({'Repository Title':title[:18],\n",
    "                 'Repository Description':desc[:18],\n",
    "                 'Contributors Count':Counted[:18],\n",
    "                 'Language used':lang[:18],\n",
    "                 })\n",
    "               \n",
    "print(df.head(10))\n",
    "df.to_csv(\"github.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6.Scrape the details of top 100 songs on billiboard.com. Url = https://www.billiboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Song Name                                Artist Name Last week  \\\n",
      "0  Drivers License                             Olivia Rodrigo         1   \n",
      "1             Mood               24kGoldn Featuring iann dior         1   \n",
      "2  Blinding Lights                                 The Weeknd         1   \n",
      "3            34+35                              Ariana Grande         2   \n",
      "4       Levitating                  Dua Lipa Featuring DaBaby         5   \n",
      "5         Go Crazy                   Chris Brown & Young Thug         5   \n",
      "6        Positions                              Ariana Grande         1   \n",
      "7  Save Your Tears                                 The Weeknd        14   \n",
      "8             Holy  Justin Bieber Featuring Chance The Rapper         3   \n",
      "9          Whoopty                                         CJ        16   \n",
      "\n",
      "  Peak rank Weeks  \n",
      "0         1        \n",
      "1        26     1  \n",
      "2        61     1  \n",
      "3        14     4  \n",
      "4        18     2  \n",
      "5        39     1  \n",
      "6        15    26  \n",
      "7         8     3  \n",
      "8        20     1  \n",
      "9        10    61  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "driver.get('https://www.billboard.com/')\n",
    "time.sleep(3)\n",
    "\n",
    "#Click on Chart option\n",
    "driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[2]/header/div/ul/li[1]/a').click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Click on Hot 100 page option\n",
    "driver.find_element_by_xpath('/html/body/main/div[2]/div/div[1]/a/div[2]/div[2]/div[1]').click()\n",
    "time.sleep(2)\n",
    "\n",
    "#Scroll to target location\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "target = driver.find_element_by_xpath('/html/body/main/div/div/div[7]/div/ol')\n",
    "target.location_once_scrolled_into_view \n",
    "time.sleep(2)\n",
    "\n",
    "name,artist,last_rank,peak_rank,weeks = [],[],[],[],[]\n",
    "\n",
    "#Scrape required details\n",
    " \n",
    "for i in driver.find_elements_by_xpath(\"/html/body/main/div/div/div[7]/div/ol/li/button/span[2]/span[1]\"):\n",
    "    name.append(i.text)\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"/html/body/main/div/div/div[7]/div/ol/li/button/span[2]/span[2]\"):\n",
    "    artist.append(i.text)\n",
    "     \n",
    "for i in driver.find_elements_by_xpath(\"/html/body/main/div/div/div[7]/div/ol/li/button/div/div[2]\"):\n",
    "    last_rank.append(i.text)\n",
    "        \n",
    "for i in driver.find_elements_by_xpath(\"/html/body/main/div/div/div[7]/div/ol/li/button/div/div[3]\"):\n",
    "    peak_rank.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath(\"/html/body/main/div/div/div[7]/div/ol/li/button/div/div\"):\n",
    "    weeks.append(i.text)\n",
    "\n",
    "driver.quit()\n",
    "#print(len(name),len(artist),len(last_rank),len(peak_rank),len(weeks))\n",
    "#Saving in Dataframe\n",
    "df=pd.DataFrame({'Song Name':name[:100],\n",
    "                 'Artist Name':artist[:100],\n",
    "                 'Last week':last_rank[:100],\n",
    "                 'Peak rank':peak_rank[:100],\n",
    "                 'Weeks':weeks[:100]})\n",
    "\n",
    "print(df.head(10))\n",
    "df.to_csv(\"Billboard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7.Scrape the details of Data science recruiters from naukri.com. Url = https://www.naukri.com/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Designation\n",
    "C) Company\n",
    "D) Skills they hire for\n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Name                  Designation  \\\n",
      "0                                   Aakash Harit                   HR Manager   \n",
      "1                   Talent Acquisition Executive     Recruitment Professional   \n",
      "2                       MARSIAN Technologies LLP                   Company HR   \n",
      "3                                 Jitendra Singh  Manager- Talent Acquisition   \n",
      "4  Institute for Financial Management and Resear            Programme Manager   \n",
      "5                                  Asif Lucknowi                     Director   \n",
      "6                                Kalpana Dumpala             Executive Hiring   \n",
      "7                                 Kushal Rastogi                   Company HR   \n",
      "8                                   Kapil Devang                   HR Manager   \n",
      "9                             Mahesh Babu Channa                 HR Team Lead   \n",
      "\n",
      "                               Company                  Location  \\\n",
      "0                 Data Science Network                     Delhi   \n",
      "1                           XenonStack                Chandigarh   \n",
      "2             MARSIAN Technologies LLP                      Pune   \n",
      "3  Compunnel Technology India Pvt. Ltd                     Delhi   \n",
      "4                                 IFMR                   Chennai   \n",
      "5           Weupskill- Live Wire India                    Indore   \n",
      "6                   Innominds Software  Hyderabad / Secunderabad   \n",
      "7   QuantMagnum Technologies Pvt. Ltd.                    Mumbai   \n",
      "8                       BISP Solutions                    Bhopal   \n",
      "9                    SocialPrachar.com  Hyderabad / Secunderabad   \n",
      "\n",
      "                                              Skills  \n",
      "0  Classic ASP Developer, Internet Marketing Prof...  \n",
      "1  Web Designing, html5, Angular.js, seo, hadoop,...  \n",
      "2  Data Science, Artificial Intelligence, Machine...  \n",
      "3  Python, Data Science, .Net, Java, Big Data, Da...  \n",
      "4                                       Data Science  \n",
      "5  Technical Training, Software Development, Pres...  \n",
      "6  Qa, Ui/ux, Java Developer, Java Architect, C++...  \n",
      "7  Office Administration, Hr Administration, tele...  \n",
      "8     Big Data, Hadoop, Data Analytics, Data Science  \n",
      "9  Social Media, digital media maketing, seo, smm...  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "driver.get('https://www.naukri.com/hr-recruiters-consultants')\n",
    "time.sleep(3)\n",
    "\n",
    "#driver.find_element_by_xpath('/html/body/div[1]/div[1]/div/ul[1]/li[2]/a/div').click()\n",
    "#time.sleep(3)\n",
    "#driver.switch_to.window('hr-recruiters-consultants');\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[1]/div[1]/form/div[1]/div/div[1]/div[1]/div[2]/input\").send_keys(\"Data Science\")\n",
    "time.sleep(3)\n",
    "\n",
    "# clicking the search button\n",
    "driver.find_element_by_xpath('//button[@class=\"fl qsbSrch blueBtn\"]').click()\n",
    "time.sleep(2)\n",
    "\n",
    "name,designation,company,skills,location = [],[],[],[],[]\n",
    "#Scrape required details\n",
    " \n",
    "for i in driver.find_elements_by_xpath(\"/html/body/div[3]/div/div[2]/div/div[2]/div/div[1]/div[1]/div[1]/p/a[1]/span\"):\n",
    "    name.append(i.text)\n",
    "\n",
    "for i in driver.find_elements_by_xpath(\"/html/body/div[3]/div/div[2]/div/div[2]/div/div[1]/div[1]/div[1]/p/span[1]\"):\n",
    "    designation.append(i.text)\n",
    "     \n",
    "for i in driver.find_elements_by_xpath(\"/html/body/div[3]/div/div[2]/div/div[2]/div/div[1]/div[1]/div[1]/p/a[2]\"):\n",
    "    company.append(i.text)\n",
    "        \n",
    "for i in driver.find_elements_by_xpath(\"/html/body/div[3]/div/div[2]/div/div[2]/div/div[1]/div[1]/div[1]/p/span[2]/small\"):\n",
    "    location.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath(\"/html/body/div[3]/div/div[2]/div/div[2]/div/div[1]/div[1]/div[2]\"):\n",
    "    skills.append(i.text)\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "#print(len(name),len(designation),len(company),len(location),len(skills))\n",
    "#Saving in Dataframe\n",
    "df=pd.DataFrame({'Name':name[:50],\n",
    "                 'Designation':designation[:50],\n",
    "                 'Company':company[:50],\n",
    "                 'Location':location[:50],\n",
    "                 'Skills':skills[:50]})\n",
    "\n",
    "print(df.head(10))\n",
    "df.to_csv(\"Naukri.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8.Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Book Name       Author Name  \\\n",
      "0                                   Da Vinci Code,The        Brown, Dan   \n",
      "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
      "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
      "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
      "4                                Fifty Shades of Grey      James, E. L.   \n",
      "..                                                ...               ...   \n",
      "95                                          Ghost,The    Harris, Robert   \n",
      "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
      "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
      "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
      "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
      "\n",
      "   Volumes Sold        Publisher                        Genre  \n",
      "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
      "1     4,475,152       Bloomsbury           Children's Fiction  \n",
      "2     4,200,654       Bloomsbury           Children's Fiction  \n",
      "3     4,179,479       Bloomsbury           Children's Fiction  \n",
      "4     3,758,936     Random House              Romance & Sagas  \n",
      "..          ...              ...                          ...  \n",
      "95      807,311     Random House   General & Literary Fiction  \n",
      "96      794,201          Penguin        Food & Drink: General  \n",
      "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
      "98      791,507            Orion           Biography: General  \n",
      "99      791,095          Penguin        Food & Drink: General  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "\n",
    "#Scroll to target location\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "target = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[2]/div/div[2]/h2[1]')\n",
    "target.location_once_scrolled_into_view \n",
    "time.sleep(2)\n",
    "\n",
    "#Another method for scraping table:\n",
    "#soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#for tr in soup.find(class_=\"in-article sortable\").find_all(\"tr\"):\n",
    "#    data = [item.get_text(strip=True) for item in tr.find_all([\"th\",\"td\"])]\n",
    "#    print(data)\n",
    "\n",
    "b_name,a_name,vol_sold,publisher,genre= [],[],[],[],[]  \n",
    "\n",
    "#Scrape required details\n",
    "for i in driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr/td[2]'):\n",
    "    b_name.append(i.text)\n",
    "                \n",
    "for i in driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr/td[3]'):\n",
    "    a_name.append(i.text)\n",
    "    \n",
    "for i in driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr/td[4]'):\n",
    "    vol_sold.append(i.text)\n",
    "        \n",
    "for i in driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr/td[5]'):\n",
    "    publisher.append(i.text)\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//table[@class=\"in-article sortable\"]/tbody/tr/td[6]'):\n",
    "    genre.append(i.text)\n",
    "\n",
    "driver.quit()\n",
    "#Saving in Dataframe\n",
    "df=pd.DataFrame({'Book Name' : b_name,\n",
    "                 'Author Name' : a_name,\n",
    "                 'Volumes Sold' : vol_sold,\n",
    "                 'Publisher' : publisher,\n",
    "                 'Genre' : genre })\n",
    "print(df)\n",
    "df.to_csv('guru99.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9.Scrape the details most watched tv series of all time from imdb.com. Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Name         Year                     Genre  \\\n",
      "0                  Game of Thrones  (2011–2019)  Action, Adventure, Drama   \n",
      "1                  Stranger Things     (2016– )    Drama, Fantasy, Horror   \n",
      "2                 The Walking Dead     (2010– )   Drama, Horror, Thriller   \n",
      "3                   13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller   \n",
      "4                          The 100  (2014–2020)    Drama, Mystery, Sci-Fi   \n",
      "..                             ...          ...                       ...   \n",
      "95                           Reign  (2013–2017)            Drama, Fantasy   \n",
      "96  A Series of Unfortunate Events  (2017–2019)  Adventure, Comedy, Drama   \n",
      "97                  Criminal Minds  (2005–2020)     Crime, Drama, Mystery   \n",
      "98           Scream: The TV Series  (2015–2019)      Crime, Drama, Horror   \n",
      "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
      "\n",
      "   Run_time Ratings      Votes  \n",
      "0    57 min     9.3  1,769,018  \n",
      "1    51 min     8.7    821,447  \n",
      "2    44 min     8.2    852,986  \n",
      "3    60 min     7.6    256,085  \n",
      "4    43 min     7.6    216,301  \n",
      "..      ...     ...        ...  \n",
      "95   42 min     7.5     43,563  \n",
      "96   50 min     7.8     53,845  \n",
      "97   42 min     8.1    161,082  \n",
      "98   45 min     7.2     34,081  \n",
      "99  572 min     8.6    182,161  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "driver.get('https://www.imdb.com/list/ls095964455/')\n",
    "time.sleep(3)\n",
    "\n",
    "imdb_dict = {}\n",
    "imdb_dict['Name']= []\n",
    "imdb_dict['Year']= []\n",
    "imdb_dict['Genre']= []\n",
    "imdb_dict['Run_time']= []\n",
    "imdb_dict['Ratings']= []\n",
    "imdb_dict['Votes']= []\n",
    "\n",
    "#Scrape required details \n",
    "name = driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/a\")\n",
    "for i in name:\n",
    "    imdb_dict['Name'].append(i.text)\n",
    "                \n",
    "year=driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/span[2]\")\n",
    "for i in year:\n",
    "    imdb_dict['Year'].append(i.text)\n",
    "    \n",
    "genre = driver.find_elements_by_xpath(\"//p[@class='text-muted text-small']/span[5]\")\n",
    "for i in genre:\n",
    "    imdb_dict['Genre'].append(i.text)\n",
    "        \n",
    "run_time = driver.find_elements_by_xpath(\"//p[@class='text-muted text-small']/span[3]\")\n",
    "for i in run_time:\n",
    "    imdb_dict['Run_time'].append(i.text)\n",
    "    \n",
    "ratings = driver.find_elements_by_xpath(\"//div[@class='ipl-rating-star small']/span[2]\")\n",
    "for i in ratings:\n",
    "    imdb_dict['Ratings'].append(i.text)\n",
    "    \n",
    "votes = driver.find_elements_by_xpath(\"//div[@class='lister-item-content']/p[4]/span[2]\")\n",
    "for i in votes:\n",
    "    imdb_dict['Votes'].append(i.text)\n",
    "    \n",
    "driver.close()   \n",
    "#print(len(Name),len(Year),len(Genre),len(Run_time),len(Ratings),len(Votes))\n",
    "#Saving in dataframe            \n",
    "imdb_df=pd.DataFrame(imdb_dict)\n",
    "print(imdb_df)\n",
    "imdb_df.to_csv('imdb.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Details of Datasets from UCI machine learning repositories. Url = https://archive.ics.uci.edu/\n",
    "You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Dataset Name      Data Type               Attribute Type  \\\n",
      "0                         Adult  Multivariate   Categorical, Integer, Real    \n",
      "1                     Annealing  Multivariate         Categorical, Integer    \n",
      "2  Anonymous Microsoft Web Data  Multivariate   Categorical, Integer, Real    \n",
      "3                    Arrhythmia                                Categorical    \n",
      "4         Artificial Characters  Multivariate   Categorical, Integer, Real    \n",
      "5          Audiology (Original)  Multivariate   Categorical, Integer, Real    \n",
      "6      Audiology (Standardized)  Multivariate                  Categorical    \n",
      "7                      Auto MPG  Multivariate                  Categorical    \n",
      "8                    Automobile  Multivariate            Categorical, Real    \n",
      "9                        Badges  Multivariate   Categorical, Integer, Real    \n",
      "\n",
      "                   Task   Year Instances                   Attributes  \n",
      "0       Classification   1995      4177   Categorical, Integer, Real   \n",
      "1       Classification   1996     48842         Categorical, Integer   \n",
      "2       Classification              798   Categorical, Integer, Real   \n",
      "3  Recommender-Systems   1998     37711                  Categorical   \n",
      "4       Classification   1998       452   Categorical, Integer, Real   \n",
      "5       Classification   1992      6000   Categorical, Integer, Real   \n",
      "6       Classification   1987       226                  Categorical   \n",
      "7       Classification   1992       226                  Categorical   \n",
      "8           Regression   1993       398            Categorical, Real   \n",
      "9           Regression   1987       205   Categorical, Integer, Real   \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\home\\chromedriver.exe\")\n",
    "driver.get(\"https://archive.ics.uci.edu/\")\n",
    "driver.maximize_window()\n",
    "time.sleep(3)\n",
    "\n",
    "#Click on All Datasets option\n",
    "driver.find_element_by_xpath('/html/body/table[1]/tbody/tr/td[2]/span[2]/a/font/b').click()\n",
    "time.sleep(3)    \n",
    "\n",
    "name,d_type,task,a_type,instances,attribute,year =[],[],[],[],[],[],[]\n",
    "\n",
    "#Scrape required details \n",
    "Name = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[1]/table/tbody/tr/td[2]/p/b/a\")\n",
    "for i in Name:\n",
    "    if i.text is None :\n",
    "        name.append(\"--\")\n",
    "    else:\n",
    "        name.append(i.text)\n",
    "                \n",
    "Year=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]/p\")\n",
    "for i in Year:\n",
    "    if i.text is None :\n",
    "        year.append(\"--\")\n",
    "    else:\n",
    "        year.append(i.text)\n",
    "    \n",
    "D_type = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]/p\")\n",
    "for i in D_type:\n",
    "    if i.text is None :\n",
    "        d_type.append(\"--\")\n",
    "    else:\n",
    "        d_type.append(i.text)\n",
    "\n",
    "A_type = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]/p\")\n",
    "for i in A_type:\n",
    "    if i.text is None :\n",
    "        a_type.append(\"--\")\n",
    "    else:\n",
    "        a_type.append(i.text)\n",
    "        \n",
    "Task = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]/p\")\n",
    "for i in Task:\n",
    "    if i.text is None :\n",
    "        task.append(\"--\")\n",
    "    else:\n",
    "        task.append(i.text)\n",
    "    \n",
    "Instances = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]/p\")\n",
    "for i in Instances:\n",
    "    if i.text is None :\n",
    "        instances.append(\"--\")\n",
    "    else:\n",
    "        instances.append(i.text)\n",
    "    \n",
    "Attribute = driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]/p\")\n",
    "for i in Attribute:\n",
    "    if i.text is None :\n",
    "        attribute.append(\"--\")\n",
    "    else:\n",
    "        attribute.append(i.text)\n",
    "\n",
    "driver.quit()\n",
    "#print(len(name),len(d_type),len(year),len(instances),len(attribute),len(task),len(a_type),len(d_type))\n",
    "#Saving in Dataframe\n",
    "df=pd.DataFrame({'Dataset Name':name[1:100],\n",
    "                 'Data Type':d_type[1:100],\n",
    "                 'Attribute Type':a_type[1:100],\n",
    "                 'Task':task[1:100],\n",
    "                 'Year':year[1:100],\n",
    "                 'Instances':instances[1:100],\n",
    "                 'Attributes':attribute[1:100]})\n",
    "\n",
    "print(df.head(10))\n",
    "df.to_csv(\"Dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
